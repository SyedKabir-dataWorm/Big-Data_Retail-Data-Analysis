{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retail Analysis  Solution\n",
    "\n",
    "## Author : Syed Kabir \n",
    "\n",
    "## Table of Contents\n",
    "* \n",
    "    * [Part 1 : Working with RDD](#part-1)\n",
    "        * [1.1 Data Preparation and Loading](#1.1)\n",
    "        * [1.2 Data Partitioning in RDD](#1.2)\n",
    "        * [1.3 Query/Analysis](#1.3)\n",
    "    * [Part 2 : Working with DataFrames](#2-dataframes)\n",
    "        * [2.1 Data Preparation and Loading](#2-dataframes)\n",
    "        * [2.2 Query/Analysis](#2.2)\n",
    "    * [Part 3 :  RDDs vs DataFrame vs Spark SQL](#part-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 : Working with RDDs <a class=\"anchor\" name=\"part-1\"></a>\n",
    "## 1. Working with RDD\n",
    "In this section, RDDs are being created from the given datasets, partitioning in these RDDs are being performed and also perform use of various RDD operations to answer the queries for retail analysis. \n",
    "\n",
    "### 1.1 Data Preparation and Loading <a class=\"anchor\" name=\"1.1\"></a>\n",
    "#### 1.1.1: Codes are wriiten to create a SparkContext object using SparkSession, which tells Spark how to access a cluster. To create a SparkSession, at first SparkConf object is built that contains information about the application, using Melbourne time as the session timezone. An appropriate name for the application is given and Spark is run locally with as many working processors as logical cores available in the machine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SparkConf class into program\n",
    "from pyspark import SparkConf\n",
    "\n",
    "# local[*]: run Spark in local mode with as many working processors as logical cores on your machine\n",
    "master = \"local[*]\"\n",
    "\n",
    "# The `appName` field is a name to be shown on the Spark cluster UI page\n",
    "app_name = \"Retail Analysis App\"\n",
    "\n",
    "# Setup configuration parameters for Spark\n",
    "spark_conf = SparkConf(). \\\n",
    "                setMaster(master).setAppName(app_name). \\\n",
    "                set(\"spark.sql.session.timeZone\", \"Australia/Melbourne\") # setting the timezone for the session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SparkSession classes\n",
    "from pyspark.sql import SparkSession # Spark SQL\n",
    "\n",
    "# Method : Using SparkSession; Configuring time zone\n",
    "spark = SparkSession.builder. \\\n",
    "                     config(conf=spark_conf). \\\n",
    "                     getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2 : Load the features, sales and stores csv file into features, sales and stores RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_rdd_raw = sc.textFile('features.csv')\n",
    "sales_rdd_raw = sc.textFile('sales.csv')\n",
    "stores_rdd_raw = sc.textFile('stores.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2 For each features, sales and stores RDDs, remove the header rows and display the total count and first 10 records. Hint : You can use csv.reader to parse rows in RDDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows in featutures_rdd: 8190\n",
      "First 10 rows of featutures_rdd : \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['1,05/02/2010,42.31,2.572,NA,NA,NA,NA,NA,211.0963582,8.106,FALSE',\n",
       " '1,12/02/2010,38.51,2.548,NA,NA,NA,NA,NA,211.2421698,8.106,TRUE',\n",
       " '1,19/02/2010,39.93,2.514,NA,NA,NA,NA,NA,211.2891429,8.106,FALSE',\n",
       " '1,26/02/2010,46.63,2.561,NA,NA,NA,NA,NA,211.3196429,8.106,FALSE',\n",
       " '1,05/03/2010,46.5,2.625,NA,NA,NA,NA,NA,211.3501429,8.106,FALSE',\n",
       " '1,12/03/2010,57.79,2.667,NA,NA,NA,NA,NA,211.3806429,8.106,FALSE',\n",
       " '1,19/03/2010,54.58,2.72,NA,NA,NA,NA,NA,211.215635,8.106,FALSE',\n",
       " '1,26/03/2010,51.45,2.732,NA,NA,NA,NA,NA,211.0180424,8.106,FALSE',\n",
       " '1,02/04/2010,62.27,2.719,NA,NA,NA,NA,NA,210.8204499,7.808,FALSE',\n",
       " '1,09/04/2010,65.86,2.77,NA,NA,NA,NA,NA,210.6228574,7.808,FALSE']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header_features = features_rdd_raw.first()\n",
    "features_rdd = features_rdd_raw.filter(lambda row: row != header_features)\n",
    "print(f\"Total number of rows in featutures_rdd: {features_rdd.count()}\")\n",
    "print(\"First 10 rows of featutures_rdd : \")\n",
    "features_rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows in saleses_rdd: 421570\n",
      "First 10 rows of sales_rdd : \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['1,1,05/02/2010,24924.5,FALSE',\n",
       " '1,1,12/02/2010,46039.49,TRUE',\n",
       " '1,1,19/02/2010,41595.55,FALSE',\n",
       " '1,1,26/02/2010,19403.54,FALSE',\n",
       " '1,1,05/03/2010,21827.9,FALSE',\n",
       " '1,1,12/03/2010,21043.39,FALSE',\n",
       " '1,1,19/03/2010,22136.64,FALSE',\n",
       " '1,1,26/03/2010,26229.21,FALSE',\n",
       " '1,1,02/04/2010,57258.43,FALSE',\n",
       " '1,1,09/04/2010,42960.91,FALSE']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header_sales = sales_rdd_raw.first()\n",
    "sales_rdd = sales_rdd_raw.filter(lambda row: row != header_sales)\n",
    "print(f\"Total number of rows in saleses_rdd: {sales_rdd.count()}\")\n",
    "print(\"First 10 rows of sales_rdd : \")\n",
    "sales_rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows in stores_rdd: 45\n",
      "First 10 rows of stores_rdd : \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['1,A,151315',\n",
       " '2,A,202307',\n",
       " '3,B,37392',\n",
       " '4,A,205863',\n",
       " '5,B,34875',\n",
       " '6,A,202505',\n",
       " '7,B,70713',\n",
       " '8,A,155078',\n",
       " '9,B,125833',\n",
       " '10,B,126512']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header_stores = stores_rdd_raw.first()\n",
    "stores_rdd = stores_rdd_raw.filter(lambda row: row != header_stores)\n",
    "print(f\"Total number of rows in stores_rdd: {stores_rdd.count()}\")\n",
    "print(\"First 10 rows of stores_rdd : \")\n",
    "stores_rdd.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Data Partitioning in RDD <a class=\"anchor\" name=\"1.2\"></a>\n",
    "#### 1.2.1 : How many partitions do the above RDDs have? How is the data in these RDDs partitioned by default, when we do not explicitly specify any partitioning strategy? Can you explain Why it will be partitioned in this number? Hint: searching the source code to try to understand the reason. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total partitions: 2\n"
     ]
    }
   ],
   "source": [
    "numPartitions = features_rdd.getNumPartitions()\n",
    "print(f\"Total partitions: {numPartitions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total partitions: 2\n"
     ]
    }
   ],
   "source": [
    "numPartitions = sales_rdd.getNumPartitions()\n",
    "print(f\"Total partitions: {numPartitions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total partitions: 2\n"
     ]
    }
   ],
   "source": [
    "numPartitions = stores_rdd.getNumPartitions()\n",
    "print(f\"Total partitions: {numPartitions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:black\"><strong>ANSWER:</strong> \n",
    "    From the source code, we can see that the default partition number is 2 if the function \"textfile\" did not have the minPartitions parameters. The default setting is min(self.defaultParallelism, 2)\n",
    "</p>\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/python/_modules/pyspark/context.html#SparkContext.textFile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 :  Create a key value RDD for the store RDD, use the store type as the key and all of columns as the value. Print out the first 5 records of the key-value RDD. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', ['1', '151315']),\n",
       " ('A', ['2', '202307']),\n",
       " ('B', ['3', '37392']),\n",
       " ('A', ['4', '205863']),\n",
       " ('B', ['5', '34875'])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Implement function with logic to be applied to the RDDs\n",
    "def parseRecord(line):\n",
    "    # Split line separated by comma\n",
    "    array_line = line.split(',')\n",
    "    # Return a tuple with the car model as first element and the remaining as the second element\n",
    "    return (array_line[1], [array_line[0],array_line[2]])\n",
    "\n",
    "storetypekey_rdd = stores_rdd.map(parseRecord)\n",
    "storetypekey_rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.3 : Write the code to seperate the store key-value RDD based on the store type (each type should be in the same partition). Print out the total partition's number and the number of records in each partition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_partitions(data):\n",
    "    numPartitions = data.getNumPartitions()\n",
    "    partitions = data.glom().collect()\n",
    "    print(f\"####### NUMBER OF PARTITIONS: {numPartitions}\")\n",
    "    for index, partition in enumerate(partitions):\n",
    "        # show partition if it is not empty\n",
    "        if len(partition) > 0:\n",
    "            print(f\"Partition {index}: {len(partition)} records\")\n",
    "            print(partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####### NUMBER OF PARTITIONS: 3\n",
      "Partition 0: 23 records\n",
      "[('B', ['3', '37392']), ('B', ['5', '34875']), ('B', ['7', '70713']), ('B', ['9', '125833']), ('B', ['10', '126512']), ('B', ['12', '112238']), ('B', ['15', '123737']), ('B', ['16', '57197']), ('B', ['17', '93188']), ('B', ['18', '120653']), ('B', ['21', '140167']), ('B', ['22', '119557']), ('B', ['23', '114533']), ('B', ['25', '128107']), ('B', ['29', '93638']), ('C', ['30', '42988']), ('B', ['35', '103681']), ('C', ['37', '39910']), ('C', ['38', '39690']), ('C', ['42', '39690']), ('C', ['43', '41062']), ('C', ['44', '39910']), ('B', ['45', '118221'])]\n",
      "Partition 1: 22 records\n",
      "[('A', ['1', '151315']), ('A', ['2', '202307']), ('A', ['4', '205863']), ('A', ['6', '202505']), ('A', ['8', '155078']), ('A', ['11', '207499']), ('A', ['13', '219622']), ('A', ['14', '200898']), ('A', ['19', '203819']), ('A', ['20', '203742']), ('A', ['24', '203819']), ('A', ['26', '152513']), ('A', ['27', '204184']), ('A', ['28', '206302']), ('A', ['31', '203750']), ('A', ['32', '203007']), ('A', ['33', '39690']), ('A', ['34', '158114']), ('A', ['36', '39910']), ('A', ['39', '184109']), ('A', ['40', '155083']), ('A', ['41', '196321'])]\n"
     ]
    }
   ],
   "source": [
    "# Creating a hash function to convert catagorical value to hashed value \n",
    "def hash_function(key):\n",
    "    hashed_key = hash(key)\n",
    "    return hashed_key\n",
    "\n",
    "# hash partitioning\n",
    "no_of_partitions= storetypekey_rdd.keys().distinct().count()\n",
    "hash_partitioned_rdd = storetypekey_rdd.partitionBy(no_of_partitions, hash_function)\n",
    "\n",
    "# Displaying partitioning\n",
    "print_partitions(hash_partitioned_rdd)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Query/Analysis <a class=\"anchor\" name=\"1.3\"></a>\n",
    "For this part, write relevant RDD operations to answer the following queries.\n",
    "\n",
    "**1.3.1 Calculate the average weekly sales for each year.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1,1,05/02/2010,24924.5,FALSE',\n",
       " '1,1,12/02/2010,46039.49,TRUE',\n",
       " '1,1,19/02/2010,41595.55,FALSE',\n",
       " '1,1,26/02/2010,19403.54,FALSE',\n",
       " '1,1,05/03/2010,21827.9,FALSE']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Have a look on sales rdd\n",
    "sales_rdd = sales_rdd_raw.filter(lambda row: row != header_sales)\n",
    "sales_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('05/02/2010', '24924.5'),\n",
       " ('12/02/2010', '46039.49'),\n",
       " ('19/02/2010', '41595.55'),\n",
       " ('26/02/2010', '19403.54'),\n",
       " ('05/03/2010', '21827.9')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we have to split each element separated by ',' and creating tupples by taking only values of two columns \n",
    "# Implement function with logic to be applied to the RDDs\n",
    "def parseRecord(line):\n",
    "    # Split line separated by comma\n",
    "    array_line = line.split(',')\n",
    "    # Return a tuple with 'Date' as first element and weekly sales as the second element\n",
    "    return (array_line[2], array_line[3])\n",
    "\n",
    "weeklySales_rdd = sales_rdd.map(parseRecord)\n",
    "weeklySales_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2010', '2011', '2012']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The date value is needed to be splitted by '/'for grabbing only year value\n",
    "# Taking only year component of the date\n",
    "weeklySales_year_rdd = weeklySales_rdd. \\\n",
    "                                    map(lambda x: x[0].split('/')). \\\n",
    "                                    map(lambda x: (x[2]))\n",
    "weeklySales_year_rdd.distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zipping year values with the sales value\n",
    "weeklySales_rdd1 = weeklySales_year_rdd.zip(weeklySales_rdd. \\\n",
    "                                            map(lambda x : x[1])). \\\n",
    "                                            map(lambda x: [float(i) for i in x]) # Converting string values of sales to float value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2010.0, 16270.275737033313),\n",
       " (2012.0, 15694.948597357718),\n",
       " (2011.0, 15954.070675386392)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating average weekly sales for each year\n",
    "weeklyAvgSales_rdd = weeklySales_rdd1.groupByKey(). \\\n",
    "                                        mapValues(lambda x: sum(x)/len(x))\n",
    "weeklyAvgSales_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:rgba(255,181,116,0.5);padding:10px\">\n",
    "    <strong>NOTE:</strong> Other ways are also acceptable if they can get the correct result. \n",
    "    \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3.2 Find the highest temperature record in 2011 in the 'type B' store. You should display the store ID, date, highest temperature and type in the result.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Selecting required elements from features rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1,05/02/2010,42.31,2.572,NA,NA,NA,NA,NA,211.0963582,8.106,FALSE',\n",
       " '1,12/02/2010,38.51,2.548,NA,NA,NA,NA,NA,211.2421698,8.106,TRUE',\n",
       " '1,19/02/2010,39.93,2.514,NA,NA,NA,NA,NA,211.2891429,8.106,FALSE',\n",
       " '1,26/02/2010,46.63,2.561,NA,NA,NA,NA,NA,211.3196429,8.106,FALSE',\n",
       " '1,05/03/2010,46.5,2.625,NA,NA,NA,NA,NA,211.3501429,8.106,FALSE']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's at first have a look on features_rdd\n",
    "features_rdd = features_rdd_raw.filter(lambda row: row != header_features)\n",
    "features_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', ('05/02/2010', '42.31')),\n",
       " ('1', ('12/02/2010', '38.51')),\n",
       " ('1', ('19/02/2010', '39.93')),\n",
       " ('1', ('26/02/2010', '46.63')),\n",
       " ('1', ('05/03/2010', '46.5'))]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Implement function with logic to be applied to the RDDs\n",
    "def parseRecordFeatures(line):\n",
    "    # Split line separated by comma\n",
    "    array_line = line.split(',')\n",
    "    # Return a tuple with the car model as first element and the remaining as the second element\n",
    "    return (array_line[0], (array_line[1], array_line[2]))\n",
    "\n",
    "features_rdd1 = features_rdd.map(parseRecordFeatures)\n",
    "features_rdd1.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Selecting required elements from stores rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1,A,151315', '2,A,202307', '3,B,37392', '4,A,205863', '5,B,34875']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's at first have a look on stores_rdd\n",
    "stores_rdd = stores_rdd_raw.filter(lambda row: row != header_stores)\n",
    "stores_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', 'A'), ('2', 'A'), ('3', 'B'), ('4', 'A'), ('5', 'B')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Implement function with logic to be applied to the RDDs\n",
    "def parseRecordSales(line):\n",
    "    # Split line separated by comma\n",
    "    array_line = line.split(',')\n",
    "    # Return a tuple with the store id as first element and store type as the second element\n",
    "    return (array_line[0], array_line[1])\n",
    "\n",
    "stores_rdd1 = stores_rdd.map(parseRecordSales)\n",
    "stores_rdd1.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Joining features and stores rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('4', (('05/02/2010', '43.76'), 'A')),\n",
       " ('4', (('12/02/2010', '28.84'), 'A')),\n",
       " ('4', (('19/02/2010', '36.45'), 'A')),\n",
       " ('4', (('26/02/2010', '41.36'), 'A')),\n",
       " ('4', (('05/03/2010', '43.49'), 'A'))]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Joining feature and store rdds\n",
    "feature_store_rdd = features_rdd1.join(stores_rdd1)\n",
    "feature_store_rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initial Unpacking of the elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['4', ('05/02/2010', '43.76'), 'A'],\n",
       " ['4', ('12/02/2010', '28.84'), 'A'],\n",
       " ['4', ('19/02/2010', '36.45'), 'A']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_store_rdd1 = feature_store_rdd.map(lambda x : [x[0]] +list(x[1]))\n",
    "feature_store_rdd1.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Final Unpacking Elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['4', '05/02/2010', '43.76', 'A'],\n",
       " ['4', '12/02/2010', '28.84', 'A'],\n",
       " ['4', '19/02/2010', '36.45', 'A']]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_store_rdd2 = feature_store_rdd1.map(lambda x: [x[0]] + list(x[1]) + [x[2]]) \n",
    "feature_store_rdd2.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Unpacking year data from the date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2010', '2010', '2010']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unpacking year data from the date\n",
    "feature_store_rdd3 = feature_store_rdd2.map(lambda x:  x[1].split('/')).map(lambda x: (x[2]))\n",
    "feature_store_rdd3.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2010', ['4', '05/02/2010', '43.76', 'A']),\n",
       " ('2010', ['4', '12/02/2010', '28.84', 'A']),\n",
       " ('2010', ['4', '19/02/2010', '36.45', 'A'])]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Zipping year data with the joined unpacked RDD\n",
    "feature_store_rdd4 = feature_store_rdd3.zip(feature_store_rdd2) \n",
    "feature_store_rdd4.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Rearranging and removing Date elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('4', '2010', 'A', '43.76'),\n",
       " ('4', '2010', 'A', '28.84'),\n",
       " ('4', '2010', 'A', '36.45')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rearranging and removing Date elements\n",
    "feature_store_rdd5 = feature_store_rdd4.map(lambda x: [x[0]] + x[1]).map (lambda x : (x[1], x[0], x[4], x[3]))  \n",
    "feature_store_rdd5.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Filtering for store type B and year = 2011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_store_rdd_final = feature_store_rdd5.filter(lambda x : x[2] == 'B'). \\\n",
    "                                             filter(lambda x : x[1]=='2011')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finding final rdd with maximum value of Temperature Filtering for store type B and year = 2011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('10', '2011', 'B', '95.36')\n"
     ]
    }
   ],
   "source": [
    "feature_store_rdd_max = feature_store_rdd_final.max(key=lambda x: float(x[3]))\n",
    "print(feature_store_rdd_max)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
